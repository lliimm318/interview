# kafka
오픈소스 Pub-Sub 모델의 메세지 큐이다. 분산 환경에 특화되어 있다.

데이터 파이프라인, 스트리밍 분석, 데이터 통합 및 미션 크리티컬 애플리케이션을 위한 오픈 소스 고성능 분산 이벤트 스트리밍 플랫폼이다.

## 배경
링크드인에서 만들었다. 

![image](https://user-images.githubusercontent.com/66578746/234450795-9fc5e9e7-c3a7-49c8-b3e5-220c0b8a5c68.png)

기존 링크드인 데이터 처리 시스템은 각 파이프라인이 파편화되고 시스템 복잡도가 높아서 확장이 어려웠다.

이를 해결하기 위해 아래의 목표를 가지고 새로운 시스템을 개발했다. -> 카프카
- 프로듀서와 컨슈머의 분리
- 메세징 시스템과 같이 영구 메세지 데이터를 여러 컨슈머에게 적용
- 높은 처리량을 위한 메세지 최적화
- 트래픽 증가에 따른 스케일 아웃이 가능한 시스템

![image](https://user-images.githubusercontent.com/66578746/234464457-18239dc1-957e-4005-8d5e-0fc0ab4743b0.png)

카프카를 적용하고 이렇게 변했다! 한 눈에 봐도 간결하지 않는가.

카프카를 적용해서 모든 이벤트/데이터의 흐름을 중앙(카프카)에서 관리할 수 있게 되었다

## 메세징 시스템
- Publisher와 Subscriber로 이루어진 비동기 메시징 전송 방식
- 프로듀서가 메세지 시스템에 메세제를 저장하면 해당 메세지를 컨슈머가 구독
- 프로듀서의 메세지는 수신자가 정해져 있지 않은 상태로 발행한다.
- 구독하는 컨슈머만 정해진 메세지를 받는다.
- 컨슈머는 프로듀서 정보가 없어도 메세지 수신이 가능하다.

![image](https://user-images.githubusercontent.com/66578746/234472400-8325ccda-3a9e-4935-a37b-70fbce705903.png)

일반적인 네트워크 통신이다.

N 대 N 연결이므로 클라리언트가 늘어나면 느려질 수 있고, 이를 대응하기 위한 확정이 떨어진다.
만일 특정 클라리언트가 다운되면 메세지 유실의 위험이 있다. (메세지를 받지 못함)

![image](https://user-images.githubusercontent.com/66578746/234472723-3c3b599a-7d1d-455a-a2f2-ff312cccb510.png)

Pub/Sub 모델 (발행/구독 모델)이다.

- 프로듀서가 메세지를 컨슈머에게 **직접 전달하지 않고, 중간에 메세징 시스템에 전달**한다. (수신처 ID를 포함)
- 메세징 시스템의 교환기가 수신처 ID 값을 통해 컨슈머들의 큐에 메세지를 전달한다.
- 컨슈머는 큐를 모니터링 하는데 큐에 메세지가 있는게 확인되면 값을 회수한다.

#### 장점
- 특정 개체가 수신 불가 상태가 되더라도 메세지 시스템만 살아 있으면 메세지가 유실되지 않음
- 메세징 시스템으로 연결되어 확장성에 용이

#### 단점
- 직접 통신하지 않아서 메지가 도착여부를 파악하기 힘듬
- 중간에 메세지 시스템을 거쳐서 속도가 빠르지 않음

## 기존 MQ와 차이

**디스크에 저장 (영속성)** <br/>
기존 메세지 시스템은 컨슈머가 메세지를 컨슘하면 큐에서 바로 삭제한다. 
하지만 카프카는 디스크에 메세지를 일정기간 보관해서 메세지 손실이 없다.

**배치 (속도향상)** <br/>
서버와 클라이언트 사이에서 빈번하게 발생하는 메시지 통신을 하나씩 처리할 경우 그만큼 네트워크 왕복의 오버헤드가 발생한다.
배치처리 IO가 자주 일어나는 것을 방지하기 위해 작은 IO를 그룹핑해서 처리힌다.

**분산 스트리밍 플랫폼** <br/>
시스템 확장에 용이하고 단일 시스템 대비 성능이 우수하다, 일부 노드가 죽어도 다른 노드가 해당 일을 지속한다. (고가용성)


![image](https://github.com/lliimm318/interview/assets/66578746/0ce37470-6732-46e7-b9f8-3a0eca103039)

**페이지 캐시 (속도향상)** <br/>
잔여 메모리를 이용해 디스크 read/write를 하지 않고 페이지 캐시를 통한 read/write으로 인해 처리 속도가 매우 빠르다


## 구성요소

![image](https://user-images.githubusercontent.com/66578746/234467093-0a34ea77-def8-4bd3-a549-bb875f88242a.png)

#### Topic

메세지를 논리적으로 묶은 개념

![image](https://github.com/lliimm318/interview/assets/66578746/76d98e7a-0d86-4a48-a323-e847a1fdad04)

- 메세지를 전송하거나 소비할 때 토픽이 반드시 필요하다.
- 컨슈머는 자신이 담당하는 토픽의 메시지를 처리
- 하나의 토픽에는 한 개 이상의 파티션이 있다

#### Partition

토픽을 구성하는 데이터 저장소이며 메세지가 저장되는 위치. 분산처리를 위해 사용한다

![image](https://github.com/lliimm318/interview/assets/66578746/d460f3d9-8112-4c74-ad21-6803af4e9f05)

- 토픽 생성 시 파티션 수를 지정 가능 (파티션 수는 변경 가능하나 추가만 된다.)
- 파티션이 1개면 순서가 보장된다. 그러나 여러 개라면 카프카 클러스터가 라운드 로빈으로 분산 처리되기 때문에 순서가 보장되지 않는다.
- 파티션은 여러 개의 브로커에 걸쳐 저장된다.
- 파티션 내부 각 메세지는 offset으로 구분된다.
- 여러 프로듀서에 한 파티션으로 메세지를 보내면 병목이 생기고, 메세지 순서를 보장할 수 없다. 
  파티션을 여러 개로 늘리고 그 수만큼 프로듀서도 늘려 하나의 파티션마다 하나의 프로듀서 메세지를 받으면 훨씬 빠르다.

**파티션이 늘어나면...**

  1. 파일 핸들러의 낭비가 존재
각 파티션은 브로커의 디렉토리와 매핑되고 저장되는 데이터마다 2개의 파일(인덱스, 실제 파일)이 있기 때문에 너무 많은 파일 핸들이 생길 경우 리소스 낭비

  2. 장애 복구 시간이 증가할 수 있음
카프카는 리플리케이션(Replication)을 지원하고, 이를 통해 지속적으로 리더 파티션을 팔로워 파티션으로 리플리케이션을 하게 된다.
하지만 파티션 수가 너무 많을 경우 리플리케이션 수행이 느려져 장애복구시간 증가

  3. 추후 파티션 수를 줄이는 것이 불가능
카프카에서 파티션 수를 줄이고 싶다면 토픽 자체를 삭제하는 것 말고는 방법이 없음. 늘리는 것은 아무때나 가능하지만 파티션 수를 줄이는 방법은 제공하지 않음.

#### Offset
파티션마다 메시지가 저장되는 위치

- 파티션 내에서 순차적으로 유니크하게 증가하는 숫자 형태로서 동일 파티션 내 메시지의 순서를 보장해줌
- 컨슈머는 메시지를 가져올 때마다 오프셋 정보를 커밋(commit)함으로써 기존에 어디 위치까지 가져왔는지 알 수 있게 됨
- 여러개의 파티션을 사용할 경우 동일 파티션 내에서는 순서가 보장되지만, 
  파티션과 파티션 사이에서는 순서를 보장하지 못하기 때문에 전체 메시지를 출력할 경우 순서가 섞일 수 있다.
- 전체 메시지의 순서를 보장하고 싶은 경우 partition을 1개로만 설정해야 함. 하지만 파티션이 하나이므로 분산 처리는 불가능하다.

#### replication을

고가용성 및 데이터 유실을 막기 위해 replication을 수행

![image](https://github.com/lliimm318/interview/assets/66578746/3fe152ef-e058-400b-9dc9-49a31dc03a05)

- 원본 파티션의 경우 '리더'가 되고, 복제 파티션의 경우 '팔로워'가 됨
- 리더 파티션이 있는 브로커가 다운될 경우, 복제 파티션을 가진 브로커의 팔로워 파티션이 새로운 리더가 되어 정상적으로 프로듀서의 요청을 처리

**ISR(In Sync Replica)**

리더와 팔로워로 이루어진 리플리케이션 그룹

![image](https://github.com/lliimm318/interview/assets/66578746/4c63e6d3-edfb-485c-a23e-abeae26a233f)

- 리플리케이션 그룹 내 동기화 및 신뢰성 유지
- 팔로워는 Read/Write 권한이 없고 오로지 리더로부터 데이터를 복제하기 때문에 특정 팔로워가 다운되서 리플리케이션을 못할 경우 동기화 <br/>
  이러한 문제로 인해, 문제가 감지된 팔로워(설정된 일정주기(replica.lag.time.max.ms)만큼 요청이 오지 않는 팔로워)는 ISR 그룹에서 추방

#### Producer
- 메세지를 만들어 카프카 클라스터에 전송
- 메세지 전송 시 Batch처리가 가능하
- 키 값을 지정하여 특정 파티션 전송이 가능하
- 전송 acks 값을 설정하여 효율성을 높일 수 있다.
  - CKS=0  매우 빠르게 전송 하지만, 파티션 리더가 받았는 지 알 수 없다
  - CKS=1  기본 값이며 파티션 리더가 받았는지 확인 가능
  - CKSALL 파티션 리더뿐 아니라 팔로워까지 메세지를 받았는지 확인 가능

#### Consumer
카프카 클러스터에 메세지를 읽어 처리

- 메세지를 Batch 처리 가능
- 한 개의 컨슈머는 여러 개의 토픽을 처리 가능
- 메세지를 소비해도 메세지를 삭제하지 않는다. (Kafka delete policy에 의해 삭제)
- 한 번 저장된 메세지는 여러 번 소비할 수 있다.
- 컨슈머는 컨슈머 그룹에 속하며, 파티션은 같은 컨슈머그룹의 여러 개의 컨슈머에 연결 불가

컨슈머가 메세지를 소비하는 시간보다 프로듀서가 메세지를 전달하는 속도가 더 빨라서 메세지가 점점 쌓인다면?

1. 동일 토픽에 대해 여러 컨슈머가 메세지를 가져갈 수 있는 **컨슈머 그룹**
2. ![image](https://github.com/lliimm318/interview/assets/66578746/cbc322bc-29ac-496b-92d4-028ed8908d0f)

위와 같이 하나의 컨슈머가 프로듀서의 메세지 전송 속도를 못 따라오면,

![image](https://github.com/lliimm318/interview/assets/66578746/fa8f1bca-a4a0-4177-a034-611655ee14ac)

리밸런스 - 위와 같이 컨슈머를 확장하여 하나의 파티션 당 하나의 컨슈머가 연결되도록 가능하다. 

![image](https://github.com/lliimm318/interview/assets/66578746/40ed70fa-4b31-431a-9661-c027621381c3)

이렇게는 

#### Broker
- 실행된 카프카 서버
- 프로듀서와 컨슈머는 별도의 어플리케이션으로 구성되지만, 브로커는 카프카 자체이다.
- 각 브로커는 카프카 클러스터 내부에 존재한다
- 서버 내부에 메세지를 저장하고 관리한다

#### Zookeeper
- 분산 어플리케이션 관리를 위한 코디네이션 시스템
- 분산 메세지 큐의 메타 정보를 중앙에서 관리하는 역할

## 특징
> **높은 성능 / 고가용성 / 확장성**
- 영속성 보장 (디스크에 메세지 저장)
- 프로듀서와 컨슈머를 분리하고 멀티 프로듀서와 멀티 컨슈머 지원. 컨슈머가 브로커로 부터 직접 메세지를 가져 감
- 오버해드를 줄일 수 있다 (프로듀서와 컨슈머 모두 배치 처리가 가능). 배치처리 IO가 자주 일어나는 것을 방지하기 위해 작은 IO를 그룹핑해서 처리할 수 있도록 
- 메세지 보장 여부 선택 가능
- 분산 및 복제가 쉬움
- 메세지 헤더를 지닌 TCP 기반의 프로토콜을 사용 (AMQP 프로토콜이나 JMS API를 사용하지 않음)

## 활용


